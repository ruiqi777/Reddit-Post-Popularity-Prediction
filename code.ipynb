{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Posts Popularity Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = spark.table(\"rs_v2_2006_03\").select('brand_safe','can_gild','is_crosspostable','selftext','created_utc','num_comments','no_follow','over_18',\n",
    "'author','domain','parent_whitelist_status','subreddit','subreddit_type','suggested_sort','title','score')\n",
    "# OOT data is tested by modify \"rs_v2_2006_04\" to \"rs_v2_2006_05\"\n",
    "df_test = spark.table(\"rs_v2_2006_04\").select('brand_safe','can_gild','is_crosspostable','selftext','created_utc','num_comments','no_follow','over_18',\n",
    "'author','domain','parent_whitelist_status','subreddit','subreddit_type','suggested_sort','title','score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Package installed\n",
    "!pip install nltk\n",
    "!python -m nltk.downloader all\n",
    "!pip install tldextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sentiment Analysis Function\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import format_number as fmt\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer,StopWordsRemover,Word2Vec\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import nltk\n",
    "from os import path, getcwd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def getCleanTweetText(filteredTweetText):\n",
    "    return ' '.join(filteredTweetText)\n",
    "  \n",
    "def getSentimentScore(tweetText):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    vs = analyzer.polarity_scores(tweetText)\n",
    "    return float(vs['compound'])\n",
    "\n",
    "def getSentiment(score):\n",
    "    return 1 if score > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
       "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "############## Sentiment Score for title ##############\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "import nltk.sentiment.util\n",
    "import nltk.sentiment.sentiment_analyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "senti = SentimentIntensityAnalyzer\n",
    "\n",
    "df_train = df_train.withColumn(\"title\", f.regexp_replace(f.col(\"title\"), \"[:]\", \" \").alias(\"replaced\"))\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='title', outputCol='words')\n",
    "Tokenized_title = tokenizer.transform(df_train)\n",
    "\n",
    "remover = StopWordsRemover(inputCol='words', outputCol='filteredTweetText')\n",
    "StopwordRemoved_title = remover.transform(Tokenized_title)\n",
    "\n",
    "udfCleanTweetText = udf(getCleanTweetText, StringType())\n",
    "dfFilteredCleanedTweet = StopwordRemoved_title.withColumn('filteredCleanedTweetText', udfCleanTweetText('filteredTweetText'))\n",
    "\n",
    "udfSentimentScore = udf(getSentimentScore, FloatType())\n",
    "df_train = dfFilteredCleanedTweet.withColumn('sentimentScore', udfSentimentScore('filteredCleanedTweetText')).select('sentimentScore','brand_safe','can_gild','is_crosspostable','selftext','created_utc','num_comments','no_follow','over_18',\n",
    "'author','domain','parent_whitelist_status','subreddit','subreddit_type','suggested_sort','title','score')\n",
    "# Test data\n",
    "df_test= df_test.withColumn(\"title\", f.regexp_replace(f.col(\"title\"), \"[:]\", \" \").alias(\"replaced\"))\n",
    "\n",
    "Tokenized_title2 = tokenizer.transform(df_test)\n",
    "\n",
    "StopwordRemoved_title2 = remover.transform(Tokenized_title2)\n",
    "\n",
    "dfFilteredCleanedTweet2 = StopwordRemoved_title2.withColumn('filteredCleanedTweetText', udfCleanTweetText('filteredTweetText'))\n",
    "\n",
    "df_test = dfFilteredCleanedTweet2.withColumn('sentimentScore', udfSentimentScore('filteredCleanedTweetText')).select('sentimentScore','brand_safe','can_gild','is_crosspostable','selftext','created_utc','num_comments','no_follow','over_18',\n",
    "'author','domain','parent_whitelist_status','subreddit','subreddit_type','suggested_sort','title','score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">root\n",
       "-- sentimentScore: float (nullable = true)\n",
       "-- brand_safe: boolean (nullable = true)\n",
       "-- can_gild: boolean (nullable = true)\n",
       "-- is_crosspostable: boolean (nullable = true)\n",
       "-- selftext: string (nullable = true)\n",
       "-- created_utc: long (nullable = true)\n",
       "-- num_comments: long (nullable = true)\n",
       "-- no_follow: boolean (nullable = true)\n",
       "-- over_18: boolean (nullable = true)\n",
       "-- author: string (nullable = true)\n",
       "-- domain: string (nullable = true)\n",
       "-- parent_whitelist_status: string (nullable = true)\n",
       "-- subreddit: string (nullable = true)\n",
       "-- subreddit_type: string (nullable = true)\n",
       "-- suggested_sort: string (nullable = true)\n",
       "-- title: string (nullable = true)\n",
       "-- score: long (nullable = true)\n",
       "-- day_of_week: long (nullable = true)\n",
       "-- hour: long (nullable = true)\n",
       "-- domain_converted: array (nullable = true)\n",
       "    |-- element: string (containsNull = true)\n",
       "-- cleaned_title: string (nullable = true)\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# clean up train data set\n",
    "\n",
    "# deal with utc\n",
    "from datetime import date\n",
    "import time\n",
    "def convert_utc_to_day_of_the_week(utc_stamp):\n",
    "    d = date.fromtimestamp(utc_stamp / 1000)\n",
    "    return time.strptime(d.strftime('%A'), '%A').tm_wday\n",
    "  \n",
    "def convert_utc_to_hour(utc_stamp):\n",
    "  string_hour = time.strftime('%H', time.localtime(utc_stamp))\n",
    "  return int(string_hour)\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "convert_day_udf = udf(convert_utc_to_day_of_the_week, LongType())\n",
    "convert_hour_udf = udf(convert_utc_to_hour, LongType())\n",
    "\n",
    "tmp_data_utc_week_day = df_train.withColumn(\"day_of_week\", convert_day_udf(df_train.created_utc))\n",
    "tmp_data_utc_hour = tmp_data_utc_week_day.withColumn(\"hour\", convert_hour_udf(tmp_data_utc_week_day.created_utc))\n",
    "\n",
    "# deal with domain\n",
    "\n",
    "import tldextract\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "def convert_domain(x):\n",
    "  temp = []\n",
    "  y = tldextract.extract(x)\n",
    "  if y.subdomain != '':\n",
    "    temp.append(str(y.subdomain))\n",
    "    temp.append(str(y.domain))\n",
    "  else:\n",
    "    temp.append(str(y.domain))\n",
    "    \n",
    "  return temp\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "convert_domain_udf = udf(convert_domain, ArrayType(StringType()))\n",
    "\n",
    "tmp_data_domain_converted = tmp_data_utc_hour.withColumn(\"domain_converted\", convert_domain_udf(tmp_data_utc_hour.domain))\n",
    "\n",
    "# deal with title\n",
    "\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "import re\n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(\"[.;:!/\\-'?,\\\"()\\[\\]]\")\n",
    "  \n",
    "def preprocess_reviews(reviews):\n",
    "    reviews = REPLACE_NO_SPACE.sub(\" \", reviews.lower())\n",
    "    return reviews\n",
    "\n",
    "preprocess_udf = udf(lambda y: preprocess_reviews(y))\n",
    "temp_with_cleaned_title = tmp_data_domain_converted.select('*', preprocess_udf('title').alias('cleaned_title'))\n",
    "\n",
    "temp_with_cleaned_title.printSchema()\n",
    "\n",
    "data_for_model = temp_with_cleaned_title.select('sentimentScore','brand_safe','can_gild','is_crosspostable','selftext','num_comments',\n",
    "                                               'no_follow','over_18','subreddit','day_of_week','hour','domain_converted','cleaned_title','author',\n",
    "                                                'parent_whitelist_status','subreddit_type','suggested_sort','score')\n",
    "data_for_model = data_for_model.selectExpr('sentimentScore','brand_safe','can_gild','is_crosspostable','selftext',\n",
    "                                           'num_comments','no_follow','over_18','subreddit','day_of_week','hour','domain_converted','cleaned_title','author',\n",
    "                                            'parent_whitelist_status','subreddit_type','suggested_sort','score as label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">root\n",
       "-- sentimentScore: float (nullable = true)\n",
       "-- brand_safe: boolean (nullable = true)\n",
       "-- can_gild: boolean (nullable = true)\n",
       "-- is_crosspostable: boolean (nullable = true)\n",
       "-- selftext: string (nullable = true)\n",
       "-- created_utc: long (nullable = true)\n",
       "-- num_comments: long (nullable = true)\n",
       "-- no_follow: boolean (nullable = true)\n",
       "-- over_18: boolean (nullable = true)\n",
       "-- author: string (nullable = true)\n",
       "-- domain: string (nullable = true)\n",
       "-- parent_whitelist_status: string (nullable = true)\n",
       "-- subreddit: string (nullable = true)\n",
       "-- subreddit_type: string (nullable = true)\n",
       "-- suggested_sort: string (nullable = true)\n",
       "-- title: string (nullable = true)\n",
       "-- score: long (nullable = true)\n",
       "-- day_of_week: long (nullable = true)\n",
       "-- hour: long (nullable = true)\n",
       "-- domain_converted: array (nullable = true)\n",
       "    |-- element: string (containsNull = true)\n",
       "-- cleaned_title: string (nullable = true)\n",
       "\n",
       "root\n",
       "-- sentimentScore: float (nullable = true)\n",
       "-- brand_safe: boolean (nullable = true)\n",
       "-- can_gild: boolean (nullable = true)\n",
       "-- is_crosspostable: boolean (nullable = true)\n",
       "-- selftext: string (nullable = true)\n",
       "-- num_comments: long (nullable = true)\n",
       "-- no_follow: boolean (nullable = true)\n",
       "-- over_18: boolean (nullable = true)\n",
       "-- subreddit: string (nullable = true)\n",
       "-- day_of_week: long (nullable = true)\n",
       "-- hour: long (nullable = true)\n",
       "-- domain_converted: array (nullable = true)\n",
       "    |-- element: string (containsNull = true)\n",
       "-- cleaned_title: string (nullable = true)\n",
       "-- author: string (nullable = true)\n",
       "-- parent_whitelist_status: string (nullable = true)\n",
       "-- subreddit_type: string (nullable = true)\n",
       "-- suggested_sort: string (nullable = true)\n",
       "-- label: long (nullable = true)\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# clean up test data set\n",
    "\n",
    "# deal with utc\n",
    "from datetime import date\n",
    "import time\n",
    "def convert_utc_to_day_of_the_week(utc_stamp):\n",
    "    d = date.fromtimestamp(utc_stamp)\n",
    "    return time.strptime(d.strftime('%A'), '%A').tm_wday\n",
    "  \n",
    "def convert_utc_to_hour(utc_stamp):\n",
    "  string_hour = time.strftime('%H', time.localtime(utc_stamp))\n",
    "  return int(string_hour)\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "convert_day_udf = udf(convert_utc_to_day_of_the_week, LongType())\n",
    "convert_hour_udf = udf(convert_utc_to_hour, LongType())\n",
    "\n",
    "tmp_data_utc_week_day_test = df_test.withColumn(\"day_of_week\", convert_day_udf(df_test.created_utc))\n",
    "tmp_data_utc_hour_test = tmp_data_utc_week_day_test.withColumn(\"hour\", convert_hour_udf(tmp_data_utc_week_day_test.created_utc))\n",
    "\n",
    "# deal with domain\n",
    "\n",
    "import tldextract\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "def convert_domain(x):\n",
    "  temp = []\n",
    "  y = tldextract.extract(x)\n",
    "  if y.subdomain != '':\n",
    "    temp.append(str(y.subdomain))\n",
    "    temp.append(str(y.domain))\n",
    "  else:\n",
    "    temp.append(str(y.domain))\n",
    "    \n",
    "  return temp\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "convert_domain_udf = udf(convert_domain, ArrayType(StringType()))\n",
    "\n",
    "tmp_data_domain_converted_test = tmp_data_utc_hour_test.withColumn(\"domain_converted\", convert_domain_udf(tmp_data_utc_hour_test.domain))\n",
    "\n",
    "# deal with title\n",
    "\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "import re\n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(\"[.;:!/\\-'?,\\\"()\\[\\]]\")\n",
    "\n",
    "def preprocess_reviews(reviews):\n",
    "    reviews = REPLACE_NO_SPACE.sub(\" \", reviews.lower())\n",
    "    return reviews\n",
    "\n",
    "preprocess_udf = udf(lambda y: preprocess_reviews(y))\n",
    "temp_with_cleaned_title_test = tmp_data_domain_converted_test.select('*', preprocess_udf('title').alias('cleaned_title'))\n",
    "\n",
    "temp_with_cleaned_title_test.printSchema()\n",
    "\n",
    "data_for_model_test = temp_with_cleaned_title_test.select('sentimentScore','brand_safe','can_gild','is_crosspostable','selftext','num_comments','no_follow','over_18','subreddit','day_of_week','hour','domain_converted','cleaned_title','author','parent_whitelist_status','subreddit_type','suggested_sort','score')\n",
    "data_for_model_test = data_for_model_test.selectExpr('sentimentScore','brand_safe','can_gild','is_crosspostable','selftext','num_comments','no_follow','over_18','subreddit','day_of_week','hour','domain_converted','cleaned_title','author','parent_whitelist_status','subreddit_type','suggested_sort','score as label')\n",
    "data_for_model_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+--------------------+-----+-------------------+\n",
       "            features|label|         prediction|\n",
       "+--------------------+-----+-------------------+\n",
       "(228,[8,15,16,22,...|    4|  8.711243222905392|\n",
       "(228,[8,22,24,45,...|    3|  5.628964262947462|\n",
       "(228,[8,12,15,16,...|    0|0.22495864757030884|\n",
       "(228,[8,15,16,17,...|    3| 17.695492838085467|\n",
       "(228,[8,12,21,22,...|    0| 0.8878610811666252|\n",
       "(228,[8,15,16,22,...|    0|0.24757685784805225|\n",
       "(228,[8,15,16,22,...|    0|0.27413582988984564|\n",
       "(228,[8,15,16,22,...|    0|0.25086012645321315|\n",
       "(228,[8,15,16,21,...|    0|0.21600447902329706|\n",
       "(228,[8,22,24,26,...|    3|  1.255180323821031|\n",
       "(228,[8,15,16,22,...|    0|  5.149059322396761|\n",
       "(228,[8,15,16,22,...|    1|0.24623410230251602|\n",
       "(228,[8,15,16,22,...|    6|  9.257491726589455|\n",
       "(228,[8,15,16,22,...|    0|0.24623410230251602|\n",
       "(228,[8,22,24,26,...|    2| 1.1633515062620803|\n",
       "(228,[8,15,16,22,...|    0|0.25009428200729455|\n",
       "(228,[8,22,26,27,...|   25|  7.413906952154913|\n",
       "(228,[8,15,16,22,...|  192| 110.02966673961166|\n",
       "(228,[7,8,22,26,2...|    3| 1.0673202265252846|\n",
       "(228,[7,8,22,26,2...|   17| 1.0673202265252846|\n",
       "(228,[7,8,22,26,2...|    0| 1.0673202265252846|\n",
       "(228,[8,15,16,22,...|   18|  40.79732765963166|\n",
       "(228,[8,15,16,22,...|    3|0.24623410230251602|\n",
       "(228,[8,16,22,26,...|    3|  5.460590964171746|\n",
       "(228,[8,15,16,21,...|    0|0.21651173045266536|\n",
       "(228,[8,15,16,21,...|    0|0.22033262341961174|\n",
       "(228,[8,15,16,22,...|    0|0.24988758785030313|\n",
       "(228,[8,15,16,21,...|    0|0.22033262341961174|\n",
       "(228,[8,15,16,22,...|   96| 58.084742364727774|\n",
       "(228,[8,15,16,21,...|    0|0.22033262341961174|\n",
       "(228,[8,15,16,22,...|   90|  32.64703198511485|\n",
       "(228,[8,15,16,22,...|    0| 0.5599018981231935|\n",
       "(228,[8,16,22,26,...|    2|  2.511905000772115|\n",
       "(228,[8,15,16,22,...|    0| 0.2609455297754817|\n",
       "(228,[8,22,24,26,...|    0| 1.2895620567495976|\n",
       "(228,[8,15,16,22,...|   28| 10.443783021429677|\n",
       "(228,[8,15,16,22,...|   65| 188.27674988678544|\n",
       "(228,[8,15,16,22,...|    0|0.24623410230251602|\n",
       "(228,[8,15,16,22,...|    0| 0.4481188579845516|\n",
       "(228,[8,15,16,22,...|    0| 0.2254414327482273|\n",
       "(228,[8,15,16,22,...|    0|0.24623410230251602|\n",
       "(228,[8,15,16,22,...|    0| 0.2577622043484932|\n",
       "(228,[8,16,22,26,...|    2|  9.038460620734778|\n",
       "(228,[8,15,16,22,...|    1|  9.260826603847445|\n",
       "(228,[5,8,15,16,2...|    0|0.25086012645321315|\n",
       "(228,[8,15,16,22,...|    1|0.27765416658283887|\n",
       "(228,[8,15,16,22,...|    0|0.23644300049815622|\n",
       "(228,[8,15,16,22,...|    0|0.24315186125764984|\n",
       "(228,[16,17,21,26...|    0| 1.7223054138983183|\n",
       "(228,[8,12,15,16,...|    0|0.24623410230251602|\n",
       "+--------------------+-----+-------------------+\n",
       "only showing top 50 rows\n",
       "\n",
       "Root Mean Squared Error (RMSE) on test data = 18.6979\n",
       "R2 is = 0.593891\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model fitting and evaluation\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor, LinearRegression\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.feature import FeatureHasher, RegexTokenizer, StopWordsRemover, HashingTF, IDF, Tokenizer, VectorAssembler\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and rf.\n",
    "hashingTF = HashingTF(inputCol=\"domain_converted\", outputCol=\"rawDomain\", numFeatures = 64) # numFeatures\n",
    "idf = IDF(inputCol=\"rawDomain\", outputCol=\"domain_vector\")\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"cleaned_title\", outputCol=\"words_title\", pattern=\"\\\\W\")\n",
    "remover = StopWordsRemover(inputCol=\"words_title\", outputCol=\"filtered_title\")\n",
    "hashingTF2 = HashingTF(inputCol=\"filtered_title\", outputCol=\"rawTitle\", numFeatures = 64) # numFeatures\n",
    "idf2 = IDF(inputCol=\"rawTitle\", outputCol=\"title_vector\")\n",
    "hasher = FeatureHasher(inputCols=['sentimentScore','brand_safe','can_gild','is_crosspostable','selftext','no_follow', 'over_18','subreddit', 'hour','num_comments', 'author','parent_whitelist_status','subreddit_type','suggested_sort'],\n",
    "                       outputCol=\"other_features_vector\")\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols = ['other_features_vector','title_vector','domain_vector'], outputCol = 'features')\n",
    "\n",
    "(trainingData, testData) = (data_for_model,data_for_model_test)\n",
    "\n",
    "\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# TrainValidationSplit will try all combinations of values and determine best model using\n",
    "# the evaluator.\n",
    "\n",
    "# For RandomForestRegressor Only\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol = 'label')\n",
    "\n",
    "pipeline = Pipeline(stages=[hashingTF, idf,regexTokenizer,remover,hashingTF2,idf2, hasher, vectorAssembler, rf])\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(hasher.numFeatures, [50,100])\\\n",
    "    .addGrid(rf.numTrees, [10,20,40])\\\n",
    "    .addGrid(rf.maxDepth, [3,5,10])\\\n",
    "    .build()\n",
    "\n",
    "###########################################################\n",
    "# For LinearRegression Only\n",
    "# lr= LinearRegression(featuresCol = 'features', labelCol='label')\n",
    "#\n",
    "# pipeline = Pipeline(stages=[hashingTF, idf,regexTokenizer,remover,hashingTF2,idf2,hasher, vectorAssembler, lr])\n",
    "#\n",
    "# paramGrid = ParamGridBuilder()\\\n",
    "#    .addGrid(hasher.numFeatures, [100,200])\\\n",
    "#    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "#    .addGrid(lr.fitIntercept, [False, True])\\\n",
    "#    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    "#    .build()\n",
    "##########################################################\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,estimatorParamMaps=paramGrid,evaluator=RegressionEvaluator(),numFolds=2,parallelism=2)\n",
    "\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(trainingData)\n",
    "\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found.\n",
    "predictions = cvModel.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"features\",'label','prediction').show(50)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "rmse_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = rmse_evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "r2_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = r2_evaluator.evaluate(predictions)\n",
    "print(\"R2 is = %g\" % r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Root Mean Squared Error (RMSE) on training data: 13.947234\n",
       "Coefficient of Determination (R2) on training data: 0.770329\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluation of model on training data\n",
    "train_predictions = cvModel.transform(trainingData)\n",
    "print(\"Root Mean Squared Error (RMSE) on training data: %f\" % rmse_evaluator.evaluate(train_predictions))\n",
    "print(\"Coefficient of Determination (R2) on training data: %f\" % r2_evaluator.evaluate(train_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">     idx                      name     score\n",
       "94    94  other_features_vector_94  0.569233\n",
       "26    26  other_features_vector_26  0.083512\n",
       "79    79  other_features_vector_79  0.059312\n",
       "41    41  other_features_vector_41  0.016433\n",
       "27    27  other_features_vector_27  0.010345\n",
       "130  130           title_vector_30  0.007714\n",
       "108  108            title_vector_8  0.006182\n",
       "15    15  other_features_vector_15  0.006080\n",
       "223  223          domain_vector_59  0.005560\n",
       "219  219          domain_vector_55  0.005354\n",
       "193  193          domain_vector_29  0.005143\n",
       "196  196          domain_vector_32  0.004813\n",
       "152  152           title_vector_52  0.004313\n",
       "127  127           title_vector_27  0.004092\n",
       "117  117           title_vector_17  0.004009\n",
       "126  126           title_vector_26  0.003983\n",
       "61    61  other_features_vector_61  0.003817\n",
       "118  118           title_vector_18  0.003796\n",
       "107  107            title_vector_7  0.003683\n",
       "134  134           title_vector_34  0.003599\n",
       "125  125           title_vector_25  0.003595\n",
       "147  147           title_vector_47  0.003592\n",
       "160  160           title_vector_60  0.003415\n",
       "143  143           title_vector_43  0.003327\n",
       "36    36  other_features_vector_36  0.003274\n",
       "90    90  other_features_vector_90  0.003240\n",
       "67    67  other_features_vector_67  0.003107\n",
       "24    24  other_features_vector_24  0.003087\n",
       "81    81  other_features_vector_81  0.003076\n",
       "161  161           title_vector_61  0.003025\n",
       "213  213          domain_vector_49  0.002934\n",
       "85    85  other_features_vector_85  0.002925\n",
       "74    74  other_features_vector_74  0.002798\n",
       "23    23  other_features_vector_23  0.002760\n",
       "164  164           domain_vector_0  0.002687\n",
       "115  115           title_vector_15  0.002666\n",
       "120  120           title_vector_20  0.002610\n",
       "103  103            title_vector_3  0.002593\n",
       "34    34  other_features_vector_34  0.002586\n",
       "208  208          domain_vector_44  0.002585\n",
       "[94, 26, 79, 41, 27, 130, 108, 15, 223, 219, 193, 196, 152, 127, 117, 126, 61, 118, 107, 134, 125, 147, 160, 143, 36, 90, 67, 24, 81, 161, 213, 85, 74, 23, 164, 115, 120, 103, 34, 208]\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Feature importance \n",
    "# (To obtain importance of string features, string features should be converted to numbers individually)\n",
    "\n",
    "cvModel.bestModel.stages[-1].featureImportances\n",
    "\n",
    "# Feature importance extraction function\n",
    "def ExtractFeatureImp(featureImp, dataset, featuresCol):\n",
    "    list_extract = []\n",
    "    for i in dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "        list_extract = list_extract + dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "    varlist = pd.DataFrame(list_extract)\n",
    "    varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
    "    return(varlist.sort_values('score', ascending = False))\n",
    "  \n",
    "# Depict feature importance\n",
    "varlist = ExtractFeatureImp(cvModel.bestModel.stages[-1].featureImportances, train_predictions, \"features\").head(40)\n",
    "print(varlist)\n",
    "varidx = [x for x in varlist['idx'][0:40]]\n",
    "print(varidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[29]: &#39;\\nfrom pyspark.sql.functions import udf, col\\nfrom pyspark.sql.types import StringType\\ndef NER(Token_text: str) -&gt; str :\\n  doc = nlp(Token_text)\\n  a = \\&#39;base\\&#39;\\n  for X in doc.ents:\\n#     a += str(X.text + \\&#39; \\&#39; + X.label_ + \\&#39; \\&#39;)\\n    a += str(X.label_ + \\&#39; \\&#39;)\\n  return a\\n\\nudf_NER = udf(NER, StringType())\\ndf_domain_NER = df_train.withColumn(\\&#39;domain_NER_info\\&#39;, udf_NER(col(\\&#39;domain\\&#39;)))\\n\\ntokenizer = Tokenizer(inputCol=&#34;domain_NER_info&#34;, outputCol=&#34;domain_token&#34;)\\nTokenized_test = tokenizer.transform(df_domain_NER)\\n\\nword2Vec = Word2Vec(vectorSize=16, minCount=0, inputCol=&#34;domain_token&#34;, outputCol=&#34;domain_NEG&#34;)\\nmodel = word2Vec.fit(Tokenized_test)\\ndf_train = model.transform(Tokenized_test).drop(&#34;domain_NER_info&#34;,&#34;domain_token&#34;)\\n# Test data\\ndf_domain_NER2 = df_test.withColumn(\\&#39;domain_NER_info\\&#39;, udf_NER(col(\\&#39;domain\\&#39;)))\\n\\nTokenized_test2 = tokenizer.transform(df_domain_NER2)\\n\\nmodel = word2Vec.fit(Tokenized_test2)\\ndf_test = model.transform(Tokenized_test2).drop(&#34;domain_NER_info&#34;,&#34;domain_token&#34;)\\n&#39;</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Discarded code\n",
    "# NEG of domain\n",
    "'''\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "def NER(Token_text: str) -> str :\n",
    "  doc = nlp(Token_text)\n",
    "  a = 'base'\n",
    "  for X in doc.ents:\n",
    "#     a += str(X.text + ' ' + X.label_ + ' ')\n",
    "    a += str(X.label_ + ' ')\n",
    "  return a\n",
    "\n",
    "udf_NER = udf(NER, StringType())\n",
    "df_domain_NER = df_train.withColumn('domain_NER_info', udf_NER(col('domain')))\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"domain_NER_info\", outputCol=\"domain_token\")\n",
    "Tokenized_test = tokenizer.transform(df_domain_NER)\n",
    "\n",
    "word2Vec = Word2Vec(vectorSize=16, minCount=0, inputCol=\"domain_token\", outputCol=\"domain_NEG\")\n",
    "model = word2Vec.fit(Tokenized_test)\n",
    "df_train = model.transform(Tokenized_test).drop(\"domain_NER_info\",\"domain_token\")\n",
    "# Test data\n",
    "df_domain_NER2 = df_test.withColumn('domain_NER_info', udf_NER(col('domain')))\n",
    "\n",
    "Tokenized_test2 = tokenizer.transform(df_domain_NER2)\n",
    "\n",
    "model = word2Vec.fit(Tokenized_test2)\n",
    "df_test = model.transform(Tokenized_test2).drop(\"domain_NER_info\",\"domain_token\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[30]: &#39;\\ndf_title_NER = df_train.withColumn(\\&#39;title_NER_info\\&#39;, udf_NER(col(\\&#39;title\\&#39;)))\\n\\ntokenizer = Tokenizer(inputCol=&#34;title_NER_info&#34;, outputCol=&#34;title_token&#34;)\\nTokenized_test = tokenizer.transform(df_title_NER)\\n\\nword2Vec = Word2Vec(vectorSize=16, minCount=0, inputCol=&#34;title_token&#34;, outputCol=&#34;title_NEG&#34;)\\nmodel = word2Vec.fit(Tokenized_test)\\ndf_train = model.transform(Tokenized_test).drop(&#34;domain_NER_info&#34;,&#34;domain_token&#34;)\\n# Test data\\ndf_title_NER2 = df_test.withColumn(\\&#39;title_NER_info\\&#39;, udf_NER(col(\\&#39;title\\&#39;)))\\n\\nTokenized_test2 = tokenizer.transform(df_title_NER2)\\n\\nmodel = word2Vec.fit(Tokenized_test2)\\ndf_test = model.transform(Tokenized_test2).drop(&#34;title_NER_info&#34;,&#34;title_token&#34;)\\n&#39;</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Discarded code\n",
    "# NEG of title\n",
    "'''\n",
    "df_title_NER = df_train.withColumn('title_NER_info', udf_NER(col('title')))\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"title_NER_info\", outputCol=\"title_token\")\n",
    "Tokenized_test = tokenizer.transform(df_title_NER)\n",
    "\n",
    "word2Vec = Word2Vec(vectorSize=16, minCount=0, inputCol=\"title_token\", outputCol=\"title_NEG\")\n",
    "model = word2Vec.fit(Tokenized_test)\n",
    "df_train = model.transform(Tokenized_test).drop(\"domain_NER_info\",\"domain_token\")\n",
    "# Test data\n",
    "df_title_NER2 = df_test.withColumn('title_NER_info', udf_NER(col('title')))\n",
    "\n",
    "Tokenized_test2 = tokenizer.transform(df_title_NER2)\n",
    "\n",
    "model = word2Vec.fit(Tokenized_test2)\n",
    "df_test = model.transform(Tokenized_test2).drop(\"title_NER_info\",\"title_token\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[31]: &#39;\\n# PCA\\nfrom pyspark.ml.feature import PCA\\nfrom pyspark.ml.clustering import GaussianMixture\\nfrom pyspark.sql.types import StringType\\n\\npca = PCA(k=10, inputCol=&#34;features_vec&#34;, outputCol=&#34;pcaFeatures&#34;)\\nmodel = pca.fit(df_train)\\n\\ndf_train_pca = model.transform(df_train).select(&#34;id&#34;,&#34;score&#34;,&#34;features_vec&#34;,&#34;pcaFeatures&#34;)\\n\\n# Clustering\\ngmm = GaussianMixture(featuresCol=&#34;pcaFeatures&#34;,k=3,tol=0.001,maxIter=10,seed=10000)\\nmodel = gmm.fit(df_train_pca)\\npredictions = model.transform(df_train_pca)\\n\\n# Outlier identification\\n# Threshold value is tuned based on prediction error\\ndef outlier(x):\\n  if x[0] &gt;= 0.4:\\n    return \\&#39;no\\&#39;\\n  elif x[1] &gt;= 0.4:\\n    return \\&#39;no\\&#39;\\n  elif x[2] &gt;= 0.4:\\n    return \\&#39;no\\&#39;\\n  else:\\n    return \\&#39;yes\\&#39;\\n\\noutlier_udf = udf(outlier,StringType())\\npredictions = predictions.withColumn(&#34;outlier&#34;,outlier_udf(predictions[&#34;probability&#34;]))\\ndf_train_clear = predictions.filter(predictions[&#34;outlier&#34;] == &#34;no&#34;)\\ndf_train_clear = df_train_clear.drop(\\&#39;prediction\\&#39;,\\&#39;probability\\&#39;,\\&#39;outlier\\&#39;)\\ndf_train_clear.show(truncate=50)\\n&#39;</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Discarded code\n",
    "# PCA & Outlier Detection\n",
    "'''\n",
    "# PCA\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.clustering import GaussianMixture\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "pca = PCA(k=10, inputCol=\"features_vec\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(df_train)\n",
    "\n",
    "df_train_pca = model.transform(df_train).select(\"id\",\"score\",\"features_vec\",\"pcaFeatures\")\n",
    "\n",
    "# Clustering\n",
    "gmm = GaussianMixture(featuresCol=\"pcaFeatures\",k=3,tol=0.001,maxIter=10,seed=10000)\n",
    "model = gmm.fit(df_train_pca)\n",
    "predictions = model.transform(df_train_pca)\n",
    "\n",
    "# Outlier identification\n",
    "# Threshold value is tuned based on prediction error\n",
    "def outlier(x):\n",
    "  if x[0] >= 0.4:\n",
    "    return 'no'\n",
    "  elif x[1] >= 0.4:\n",
    "    return 'no'\n",
    "  elif x[2] >= 0.4:\n",
    "    return 'no'\n",
    "  else:\n",
    "    return 'yes'\n",
    "\n",
    "outlier_udf = udf(outlier,StringType())\n",
    "predictions = predictions.withColumn(\"outlier\",outlier_udf(predictions[\"probability\"]))\n",
    "df_train_clear = predictions.filter(predictions[\"outlier\"] == \"no\")\n",
    "df_train_clear = df_train_clear.drop('prediction','probability','outlier')\n",
    "df_train_clear.show(truncate=50)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "name": "Project_final_without_translation",
  "notebookId": 4002957738248454
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
